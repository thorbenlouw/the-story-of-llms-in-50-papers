-----

## 12-Week LLM Syllabus: A Deep Dive into the State-of-the-Art (2 Papers/Week)

| Week | Section | Key Topics & Labs | Seminal Papers (with valid arXiv links) |
| :--- | :--- | :--- | :--- |
| **Phase I: The Transformer Foundations (Weeks 1-3)** |
| 1 | **Core Transformer Architecture** | Recap of RNN/LSTM limitations (for context). Multi-Head Self-Attention derivation and complexity analysis. Transformer Encoder/Decoder structure. Positional Encodings. | **1. Attention Is All You Need** (Vaswani et al., 2017) [arXiv:1706.03762]<br>**2. Recurrent Neural Network Regularization** (Zaremba et al., 2014) [arXiv:1409.2329] *(Contrasts with the Transformer's solution to sequential issues)* |
| 2 | **Pre-training Paradigms & Models** | Causal Language Modeling (GPT). Masked Language Modeling (BERT). Unsupervised Pre-training and semi-supervised transfer learning. Tokenization (BPE, WordPiece). | **1. Improving Language Understanding by Generative Pre-Training** (Radford et al., 2018) [arXiv:1803.02157]<br>**2. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (Devlin et al., 2018) [arXiv:1810.04805] |
| 3 | **Context, Embedding, and $\text{T}5$** | Positional Encoding Deep Dive ($\text{RoPE}$ vs. Absolute). The unified text-to-text paradigm. **Lab:** Implementing a small $\text{GPT}$-style block and fine-tuning an open-source $\text{T}5$ model on a sequence-to-sequence task. | **1. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer** (Raffel et al., 2019) [arXiv:1910.10683]<br>**2. RoFormer: Enhanced Transformer with Rotary Position Embedding** (Su et al., 2021) [arXiv:2104.09866] |
| **Phase II: Scaling and Efficiency (Weeks 4-6)** |
| 4 | **Scaling Laws & Compute Optimization** | Empirical scaling of model size, dataset size, and compute budget. The *Chinchilla* optimal scaling theory. Data and model parallelization concepts (Megatron-LM). | **1. Scaling Laws for Neural Language Models** (Kaplan et al., 2020) [arXiv:2001.08361]<br>**2. Training Compute-Optimal Large Language Models** (Hoffmann et al., 2022) [arXiv:2203.15556] |
| 5 | **Mixture-of-Experts ($\text{MoE}$) Architectures** | Theory of sparse activation and conditional computation. Router mechanisms and load balancing. The efficiency trade-offs of $\text{MoE}$. **Lab:** Implementing a basic $\text{MoE}$ layer and comparing its memory usage vs. a dense layer. | **1. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity** (Fedus et al., 2021) [arXiv:2101.03961]<br>**2. $\text{LLaMA}$ 2: Open Foundation and Fine-Tuned Chat Models** (Touvron et al., 2023) [arXiv:2307.09288] *(Context for modern scaling and data curriculum)* |
| 6 | **Inference Efficiency & Hardware** | The quadratic bottleneck in inference. **$\text{KV}$ Caching**. The mechanics of **FlashAttention** (tiling and fusion). Quantization schemes (8-bit, 4-bit, $\text{GPTQ}$). | **1. Attention Is All You Need, But With A Lot Less Time** (Dao et al., 2022) - *FlashAttention* [arXiv:2205.14135]<br>**2. $\text{GPTQ}$: Accurate Post-training Quantization for Generative Pre-trained Transformers** (Frantar et al., 2022) [arXiv:2210.17323] |
| **Phase III: Alignment and Generalization (Weeks 7-9)** |
| 7 | **Instruction Tuning and Generalization** | The shift to instruction-following. Task generalization via instruction-tuning ($\text{FLAN}$). Emergent abilities (In-context learning). $\text{Multimodality}$ introduction. **Evaluation & Benchmarking**: Holistic evaluation frameworks and task diversity. **Lab:** Mini-eval comparing two open models on 3–5 $\text{HELM}$/$\text{BIG-bench}$ categories; report accuracy + calibration notes. | **1. Finetuned Language Models are Zero-Shot Learners** (Wei et al., 2021) [arXiv:2109.01657]<br>**2. Emergent Abilities of Large Language Models** (Wei et al., 2022) [arXiv:2206.07682]<br>**Additional Seminal Readings:**<br>**3. Holistic Evaluation of Language Models ($\text{HELM}$)** (Liang et al., 2022) [arXiv:2211.09110] *(Comprehensive evaluation framework across multiple scenarios and metrics)*<br>**4. Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models ($\text{BIG-bench}$)** (Srivastava et al., 2022) [arXiv:2206.04615] *(Large-scale collaborative benchmark for probing model capabilities)* |
| 8 | **Reinforcement Learning from Human Feedback ($\text{RLHF}$)** | The full $\text{RLHF}$ pipeline: $\text{SFT}$, Reward Model ($\text{RM}$), $\text{PPO}$ optimization. $\text{KL}$ divergence penalty. The role of human preference data in alignment. | **1. Training language models to follow instructions with human feedback** (Ouyang et al., 2022) [arXiv:2203.02155]<br>**2. Deep Reinforcement Learning from Human Preferences** (Christiano et al., 2017) [arXiv:1706.03741] *(The conceptual origin of $\text{RLHF}$)* |
| 9 | **Reward-Free Alignment & Safety** | **Direct Preference Optimization ($\text{DPO}$)** vs. $\text{RLHF}$. The mathematical derivation of $\text{DPO}$ as an unrolled objective. Safety, toxicity mitigation, and transparency. **Factuality & CoT-Faithfulness**: Measuring truthfulness and reasoning faithfulness. **Lab:** Fine-tuning an aligned model using $\text{DPO}$. | **1. Direct Preference Optimization: Your Language Model is Secretly a Reward Model** (Rafailov et al., 2023) [arXiv:2305.18290]<br>**2. Constitutional AI: Harmlessness from AI Feedback** (Bai et al., 2022) [arXiv:2212.08073]<br>**Additional Seminal Readings:**<br>**3. TruthfulQA: Measuring How Models Mimic Human Falsehoods** (Lin et al., 2021) [arXiv:2109.07958] *(Benchmark for evaluating factual accuracy and truthfulness in model outputs)*<br>**4. Measuring Faithfulness in Chain-of-Thought Reasoning** (Lanham et al., 2023) [arXiv:2307.13702] *(Analyzes whether CoT explanations reflect actual model reasoning)* |
| **Phase IV: Deployment, Agents, and The Future (Weeks 10-12)** |
| 10 | **Reasoning, Tool Use, and Agents** | **Chain-of-Thought ($\text{CoT}$)** prompting. Self-correction and reflection. The shift from pure generation to planning and tool use. **Multimodality (Vision–Language)**: Integration of visual and textual understanding. **Lab:** Implement a $\text{ReAct}$ agent with real API/tool calls (calculator or local weather stub) + tool-use evaluation (success rate). **Optional Lab:** Evaluate $\text{LLaVA}$ or similar open VLM on a small VQA subset. | **1. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models** (Wei et al., 2022) [arXiv:2201.11903]<br>**2. ReAct: Synergizing Reasoning and Acting in Language Models** (Yao et al., 2022) [arXiv:2210.03629]<br>**Additional Seminal Readings (Tool Use):**<br>**3. Toolformer: Language Models Can Teach Themselves to Use Tools** (Schick et al., 2023) [arXiv:2302.04761] *(Self-supervised approach for teaching LMs to use external tools)*<br>**4. Gorilla: Large Language Model Connected with Massive APIs** (Patil et al., 2023) [arXiv:2305.15334] *(Fine-tuning LLMs for accurate API calls and tool selection)*<br>**Additional Seminal Readings (Multimodality):**<br>**5. Flamingo: a Visual Language Model for Few-Shot Learning** (Alayrac et al., 2022) [arXiv:2204.14198] *(Foundational vision-language model with in-context learning capabilities)*<br>**6. Visual Instruction Tuning ($\text{LLaVA}$)** (Liu et al., 2023) [arXiv:2304.08485] *(Open-source VLM trained on multimodal instruction-following data)*<br>**7. Kosmos-2: Grounding Multimodal Large Language Models to the World** (Peng et al., 2023) [arXiv:2306.14824] *(Multimodal LLM with grounding and referring capabilities)* |
| 11 | **Retrieval-Augmented Generation ($\text{RAG}$) & Memory** | The need for external memory. Architecture of $\text{RAG}$: Retriever (dense/sparse vector search) and Generator. Trade-offs between memory, context, and knowledge. Advanced retrieval techniques. **Infrastructure, Serving, and PEFT**: Efficient fine-tuning and deployment. **Lab:** Fine-tune with $\text{LoRA}$ on a small task and serve with vLLM; measure latency vs. throughput. | **1. Retrieval-Augmented Generation for Knowledge-Intensive $\text{NLP}$ Tasks** (Lewis et al., 2020) [arXiv:2005.11472]<br>**2. Lost in the Middle: How Language Models Use Long Contexts** (Liu et al., 2023) [arXiv:2307.03172] *(Highlights the limitations of massive context in Transformers)*<br>**Additional Seminal Readings:**<br>**3. LoRA: Low-Rank Adaptation of Large Language Models** (Hu et al., 2021) [arXiv:2106.09685] *(Parameter-efficient fine-tuning via low-rank updates)*<br>**4. Efficient Memory Management for Large Language Model Serving with PagedAttention (vLLM)** (Kwon et al., 2023) [arXiv:2309.06180] *(High-throughput serving system with efficient KV cache management)*<br>**5. MemGPT: Towards LLMs as Operating Systems** (Wu et al., 2023) [arXiv:2310.08560] *(Virtual context management enabling unbounded memory for LLM agents)* |
| 12 | **Beyond the Transformer** | The $\mathcal{O}(L^2)$ challenge and the search for linear complexity. **Mamba (Selective $\text{SSM}$)**: $\mathcal{O}(L)$ scaling, fixed state memory, and hardware fusion. $\text{Hyena}$ and $\text{RetNet}$ as $\text{Transformer}$ alternatives. **Hybrid/Frontier Architectures**: Combining transformers with alternative mechanisms. Future trends: Hybrid models (e.g., Jamba) and persistent memory. | **1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces** (Gu & Dao, 2023) [arXiv:2312.00752]<br>**2. Retentive Network: Enabling Training Parallelism and Low-Cost Inference for Large Language Models** (Wu et al., 2023) [arXiv:2307.08621]<br>**Additional Seminal Readings:**<br>**3. Hyena Hierarchy: Towards Larger Convolutional Language Models** (Poli et al., 2023) [arXiv:2302.10866] *(Sub-quadratic alternative using long convolutions and gating)*<br>**4. Jamba: A Hybrid Transformer-Mamba Language Model** (Lieber et al., 2024) [arXiv:2403.19887] *(Production-scale hybrid architecture combining attention and SSMs)* |


Here is a proposed list of seminal and highly influential papers for those two extension tracks, focusing on foundational concepts and key attack methods. I have ensured the papers are verifiable and relevant to a graduate-level audience.

***

### Extension Track A: Security and Robustness of LLMs

This module focuses on the LLM threat model, specifically how models can be compromised at training time (poisoning) or inference time (adversarial attacks/jailbreaks), as well as defense mechanisms.

#### Attacks

| Paper | Focus | $\text{arXiv}$ URL |
| :--- | :--- | :--- |
| **Universal and Transferable Adversarial Attacks on Aligned $\text{LLMs}$** (Zou et al., 2023) | **Jailbreaking/Evasion:** Introduces the concept of universal, transferable adversarial suffixes (a single string that can jailbreak multiple aligned $\text{LLMs}$). | [https://arxiv.org/abs/2307.15043](https://arxiv.org/abs/2307.15043) |
| **Backdoor Attacks on Language Models** (Wallace et al., 2021) | **Data Poisoning:** Demonstrates that language models are vulnerable to backdoor attacks where adversaries can inject triggers during training that cause targeted misbehavior at test time. | [https://arxiv.org/abs/2101.05809](https://arxiv.org/abs/2101.05809) |
| **PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models** (Zou et al., 2024) | **$\text{RAG}$ Attacks:** First knowledge corruption attack on $\text{RAG}$ systems, demonstrating that injecting a few malicious texts into knowledge databases can induce $\text{LLMs}$ to generate attacker-chosen answers with 90% success rate. | [https://arxiv.org/abs/2402.07867](https://arxiv.org/abs/2402.07867) |
| **JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models** (Chao et al., 2024) | **Taxonomy/Survey:** Comprehensive survey categorizing jailbreak attacks into seven types and reviewing defense mechanisms across $\text{LLMs}$ and vision-language models. | [https://arxiv.org/abs/2407.01599](https://arxiv.org/abs/2407.01599) |

#### Defenses

| Paper | Focus | $\text{arXiv}$ URL |
| :--- | :--- | :--- |
| **Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned** (Perez et al., 2022) | **Red Teaming:** Systematic approach to discovering model vulnerabilities through adversarial testing and proactive harm reduction. | [https://arxiv.org/abs/2209.07858](https://arxiv.org/abs/2209.07858) |
| **A Watermark for Large Language Models** (Kirchenbauer et al., 2023) | **Watermarking:** Embedding detectable signatures in LLM outputs to trace generated content and prevent misuse. | [https://arxiv.org/abs/2301.10226](https://arxiv.org/abs/2301.10226) |
| **Certified Robustness to Adversarial Word Substitutions** (Jia et al., 2019) | **Certified Defense:** Foundational work on provable robustness guarantees against adversarial word substitutions using Interval Bound Propagation. | [https://arxiv.org/abs/1909.00986](https://arxiv.org/abs/1909.00986) |
| **Poisoning Language Models During Instruction Tuning** (Qi et al., 2023) | **Poisoning Mitigation:** Analyzes poisoning attacks during instruction tuning and discusses detection/mitigation strategies. | [https://arxiv.org/abs/2305.00944](https://arxiv.org/abs/2305.00944) |

***

### Extension Track B: Ethics, Bias, and Alignment Principles

This module examines the origins of bias in training data, methods for evaluation, and the philosophical and technical approaches to controlling model behavior.

| Paper | Focus | $\text{arXiv}$ URL |
| :--- | :--- | :--- |
| **On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?** (Bender et al., 2021) | **Bias/Ethics Foundation:** A highly influential paper discussing the risks associated with massive scale, including data scarcity, energy consumption, and the perpetuation of deep societal biases. | [https://dl.acm.org/doi/10.1145/3442188.3445922](https://dl.acm.org/doi/10.1145/3442188.3445922) |
| **StereoSet: Measuring Stereotypical Bias in Pretrained Language Models** (Nadeem et al., 2020) | **Bias Measurement:** Comprehensive benchmark for measuring stereotypical bias across gender, race, religion, and profession in $\text{LLMs}$ using both intrasentence and intersentence contexts. | [https://arxiv.org/abs/2004.09456](https://arxiv.org/abs/2004.09456) |
| **The Alignment Problem** (Amodei et al., 2016) | **Safety/Alignment Foundation:** Anthropic's foundational work outlining the "alignment problem" (the difficulty in ensuring that AI systems act according to human intentions) before the $\text{LLM}$ boom, setting the stage for $\text{RLHF}$. | [https://arxiv.org/abs/1606.06565](https://arxiv.org/abs/1606.06565) |
| **Constitutional AI: Harmlessness from $\text{AI}$ Feedback** (Bai et al., 2022) | **Advanced Alignment:** Introduces Constitutional AI ($\text{CAI}$) as an alternative to pure $\text{RLHF}$, demonstrating how to align models based on a set of written principles rather than solely human preference labels. | [https://arxiv.org/abs/2212.08073](https://arxiv.org/abs/2212.08073) |
